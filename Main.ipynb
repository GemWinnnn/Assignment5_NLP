{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48944a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      "A blockchain is a distributed # document 1 (Blockchain)\n",
      "Artificial intelligence (AI) refers to # document 2 (Artificial Intelligence)\n",
      "Virtual reality (VR) is a # document 3 (Virtual Reality)\n",
      "Neuroscience is the scientific study # document 4 (Neuroscience)\n",
      "A quantum computer is a # document 5 (Quantum Computing)\n",
      "\n",
      "Term Frequency Vectors:\n",
      "Document 1: {'virtual': 0, 'study': 0, 'computer': 0, 'artificial': 0, 'distributed': 0.2, '(ai)': 0, '(vr)': 0, 'the': 0, 'reality': 0, 'refers': 0, 'neuroscience': 0, 'scientific': 0, 'quantum': 0, 'to': 0, 'is': 0.2, 'intelligence': 0, 'blockchain': 0.2, 'a': 0.4}\n",
      "Document 2: {'virtual': 0, 'study': 0, 'computer': 0, 'artificial': 0.2, 'distributed': 0, '(ai)': 0.2, '(vr)': 0, 'the': 0, 'reality': 0, 'refers': 0.2, 'neuroscience': 0, 'scientific': 0, 'quantum': 0, 'to': 0.2, 'is': 0, 'intelligence': 0.2, 'blockchain': 0, 'a': 0}\n",
      "Document 3: {'virtual': 0.2, 'study': 0, 'computer': 0, 'artificial': 0, 'distributed': 0, '(ai)': 0, '(vr)': 0.2, 'the': 0, 'reality': 0.2, 'refers': 0, 'neuroscience': 0, 'scientific': 0, 'quantum': 0, 'to': 0, 'is': 0.2, 'intelligence': 0, 'blockchain': 0, 'a': 0.2}\n",
      "Document 4: {'virtual': 0, 'study': 0.2, 'computer': 0, 'artificial': 0, 'distributed': 0, '(ai)': 0, '(vr)': 0, 'the': 0.2, 'reality': 0, 'refers': 0, 'neuroscience': 0.2, 'scientific': 0.2, 'quantum': 0, 'to': 0, 'is': 0.2, 'intelligence': 0, 'blockchain': 0, 'a': 0}\n",
      "Document 5: {'virtual': 0, 'study': 0, 'computer': 0.2, 'artificial': 0, 'distributed': 0, '(ai)': 0, '(vr)': 0, 'the': 0, 'reality': 0, 'refers': 0, 'neuroscience': 0, 'scientific': 0, 'quantum': 0.2, 'to': 0, 'is': 0.2, 'intelligence': 0, 'blockchain': 0, 'a': 0.4}\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function definitions to match main.py's external modules\n",
    "def compute_tf(doc, vocab):\n",
    "    tf = defaultdict(float)\n",
    "    word_count = len(doc)\n",
    "    for word in doc:\n",
    "        if word in vocab:\n",
    "            tf[word] += 1\n",
    "    for word in tf:\n",
    "        tf[word] = tf[word] / word_count if word_count > 0 else 0\n",
    "    return {word: tf.get(word, 0) for word in vocab}\n",
    "\n",
    "def compute_idf(docs, vocab):\n",
    "    idf = {}\n",
    "    N = len(docs)\n",
    "    for word in vocab:\n",
    "        docs_with_word = sum(1 for doc in docs if word in doc)\n",
    "        idf[word] = math.log(N / (docs_with_word + 1))  # Add 1 to avoid division by zero\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(tf, idf, vocab):\n",
    "    tfidf = {}\n",
    "    for word in vocab:\n",
    "        tfidf[word] = tf[word] * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "def cosine_similarity(vec1, vec2, vocab):\n",
    "    dot_product = sum(vec1[word] * vec2[word] for word in vocab)\n",
    "    norm1 = math.sqrt(sum(val ** 2 for val in vec1.values()))\n",
    "    norm2 = math.sqrt(sum(val ** 2 for val in vec2.values()))\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Define the topics\n",
    "topics = [\n",
    "    \"Blockchain\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Virtual Reality\",\n",
    "    \"Neuroscience\",\n",
    "    \"Quantum Computing\"  # Replaced duplicate \"Virtual Reality\" with a unique topic\n",
    "]\n",
    "\n",
    "# Fetch Wikipedia content (first 5 words per topic)\n",
    "documents = []\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic, auto_suggest=False)\n",
    "        content = page.content\n",
    "        words = content.split()[:5]  # Limit to 5 words as in your code\n",
    "        limited_content = ' '.join(words)\n",
    "        documents.append(limited_content)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"DisambiguationError for {topic}: {e.options}\")\n",
    "        documents.append(\"\")  # Append empty string on error\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"PageError: {topic} not found\")\n",
    "        documents.append(\"\")  # Append empty string on error\n",
    "\n",
    "# Print documents\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"{doc} # document {i+1} ({topics[i]})\")\n",
    "\n",
    "# Tokenize and apply lowercase\n",
    "tokenized_docs = [[word.lower() for word in doc.split()] for doc in documents]\n",
    "\n",
    "# Create a set of unique words (vocabulary)\n",
    "vocabulary = set(word for doc in tokenized_docs for word in doc)\n",
    "\n",
    "# Compute the term frequency for each document\n",
    "tf_vectors = [compute_tf(doc, vocabulary) for doc in tokenized_docs]\n",
    "\n",
    "# Print TF vectors\n",
    "print(\"\\nTerm Frequency Vectors:\")\n",
    "for i, tf_vector in enumerate(tf_vectors):\n",
    "    print(f\"Document {i+1}: {tf_vector}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "408ecc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency:\n",
      "virtual: 0.9162907318741551\n",
      "study: 0.9162907318741551\n",
      "computer: 0.9162907318741551\n",
      "artificial: 0.9162907318741551\n",
      "distributed: 0.9162907318741551\n",
      "(ai): 0.9162907318741551\n",
      "(vr): 0.9162907318741551\n",
      "the: 0.9162907318741551\n",
      "reality: 0.9162907318741551\n",
      "refers: 0.9162907318741551\n",
      "neuroscience: 0.9162907318741551\n",
      "scientific: 0.9162907318741551\n",
      "quantum: 0.9162907318741551\n",
      "to: 0.9162907318741551\n",
      "is: 0.0\n",
      "intelligence: 0.9162907318741551\n",
      "blockchain: 0.9162907318741551\n",
      "a: 0.22314355131420976\n",
      "\n",
      "TF-IDF Vectors:\n",
      "Document 1: {'virtual': 0.0, 'study': 0.0, 'computer': 0.0, 'artificial': 0.0, 'distributed': 0.18325814637483104, '(ai)': 0.0, '(vr)': 0.0, 'the': 0.0, 'reality': 0.0, 'refers': 0.0, 'neuroscience': 0.0, 'scientific': 0.0, 'quantum': 0.0, 'to': 0.0, 'is': 0.0, 'intelligence': 0.0, 'blockchain': 0.18325814637483104, 'a': 0.08925742052568392}\n",
      "Document 2: {'virtual': 0.0, 'study': 0.0, 'computer': 0.0, 'artificial': 0.18325814637483104, 'distributed': 0.0, '(ai)': 0.18325814637483104, '(vr)': 0.0, 'the': 0.0, 'reality': 0.0, 'refers': 0.18325814637483104, 'neuroscience': 0.0, 'scientific': 0.0, 'quantum': 0.0, 'to': 0.18325814637483104, 'is': 0.0, 'intelligence': 0.18325814637483104, 'blockchain': 0.0, 'a': 0.0}\n",
      "Document 3: {'virtual': 0.18325814637483104, 'study': 0.0, 'computer': 0.0, 'artificial': 0.0, 'distributed': 0.0, '(ai)': 0.0, '(vr)': 0.18325814637483104, 'the': 0.0, 'reality': 0.18325814637483104, 'refers': 0.0, 'neuroscience': 0.0, 'scientific': 0.0, 'quantum': 0.0, 'to': 0.0, 'is': 0.0, 'intelligence': 0.0, 'blockchain': 0.0, 'a': 0.04462871026284196}\n",
      "Document 4: {'virtual': 0.0, 'study': 0.18325814637483104, 'computer': 0.0, 'artificial': 0.0, 'distributed': 0.0, '(ai)': 0.0, '(vr)': 0.0, 'the': 0.18325814637483104, 'reality': 0.0, 'refers': 0.0, 'neuroscience': 0.18325814637483104, 'scientific': 0.18325814637483104, 'quantum': 0.0, 'to': 0.0, 'is': 0.0, 'intelligence': 0.0, 'blockchain': 0.0, 'a': 0.0}\n",
      "Document 5: {'virtual': 0.0, 'study': 0.0, 'computer': 0.18325814637483104, 'artificial': 0.0, 'distributed': 0.0, '(ai)': 0.0, '(vr)': 0.0, 'the': 0.0, 'reality': 0.0, 'refers': 0.0, 'neuroscience': 0.0, 'scientific': 0.0, 'quantum': 0.18325814637483104, 'to': 0.0, 'is': 0.0, 'intelligence': 0.0, 'blockchain': 0.0, 'a': 0.08925742052568392}\n"
     ]
    }
   ],
   "source": [
    "# Compute the Inverse Document Frequency (IDF)\n",
    "idf = compute_idf(tokenized_docs, vocabulary)\n",
    "print(\"\\nInverse Document Frequency:\")\n",
    "for term, idf_value in idf.items():\n",
    "    print(f\"{term}: {idf_value}\")\n",
    "\n",
    "# Compute TF-IDF vectors\n",
    "tfidf_vectors = [compute_tfidf(tf, idf, vocabulary) for tf in tf_vectors]\n",
    "print(\"\\nTF-IDF Vectors:\")\n",
    "for i, tfidf_vector in enumerate(tfidf_vectors):\n",
    "    print(f\"Document {i+1}: {tfidf_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91fabcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity Between All Document Pairs:\n",
      "Similarity between Document 1 (Blockchain) and Document 2 (Artificial Intelligence): 0.0000\n",
      "Similarity between Document 1 (Blockchain) and Document 3 (Virtual Reality): 0.0453\n",
      "Similarity between Document 1 (Blockchain) and Document 4 (Neuroscience): 0.0000\n",
      "Similarity between Document 1 (Blockchain) and Document 5 (Quantum Computing): 0.1060\n",
      "Similarity between Document 2 (Artificial Intelligence) and Document 3 (Virtual Reality): 0.0000\n",
      "Similarity between Document 2 (Artificial Intelligence) and Document 4 (Neuroscience): 0.0000\n",
      "Similarity between Document 2 (Artificial Intelligence) and Document 5 (Quantum Computing): 0.0000\n",
      "Similarity between Document 3 (Virtual Reality) and Document 4 (Neuroscience): 0.0000\n",
      "Similarity between Document 3 (Virtual Reality) and Document 5 (Quantum Computing): 0.0453\n",
      "Similarity between Document 4 (Neuroscience) and Document 5 (Quantum Computing): 0.0000\n",
      "\n",
      "Most Similar Documents:\n",
      "Document 1 (Blockchain) and Document 5 (Quantum Computing)\n",
      "Cosine Similarity: 0.1060\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity for all document pairs\n",
    "print(\"\\nCosine Similarity Between All Document Pairs:\")\n",
    "max_similarity = -1\n",
    "most_similar_pair = (0, 0)\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i + 1, len(documents)):\n",
    "        similarity = cosine_similarity(tfidf_vectors[i], tfidf_vectors[j], vocabulary)\n",
    "        print(f\"Similarity between Document {i+1} ({topics[i]}) and Document {j+1} ({topics[j]}): {similarity:.4f}\")\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_pair = (i, j)\n",
    "\n",
    "# Print the most similar pair\n",
    "print(f\"\\nMost Similar Documents:\")\n",
    "print(f\"Document {most_similar_pair[0]+1} ({topics[most_similar_pair[0]]}) and Document {most_similar_pair[1]+1} ({topics[most_similar_pair[1]]})\")\n",
    "print(f\"Cosine Similarity: {max_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2ffc418",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LeaveOneOut\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Suppress convergence warnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress convergence warnings\n",
    "\n",
    "# Ensure compatible versions to avoid numpy error\n",
    "# Run: pip install numpy==1.26.4 gensim==4.3.3 wikipedia-api scikit-learn nltk\n",
    "\n",
    "# Download NLTK data (for tokenization)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the topics and labels\n",
    "topics = [\n",
    "    \"Blockchain\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Virtual Reality\",\n",
    "    \"Neuroscience\",\n",
    "    \"Quantum Computing\"\n",
    "]\n",
    "labels = list(range(len(topics)))  # Numeric labels: 0, 1, 2, 3, 4\n",
    "\n",
    "# Fetch Wikipedia content (first 5 words per topic)\n",
    "documents = []\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic, auto_suggest=False)\n",
    "        content = page.content\n",
    "        words = content.split()[:5]\n",
    "        limited_content = ' '.join(words)\n",
    "        documents.append(limited_content)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"DisambiguationError for {topic}: {e.options}\")\n",
    "        documents.append(\"\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"PageError: {topic} not found\")\n",
    "        documents.append(\"\")\n",
    "\n",
    "# Print documents\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"{doc} # document {i+1} ({topics[i]})\")\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Load pre-trained Word2Vec model (Mikolov et al.)\n",
    "try:\n",
    "    # Download from: https://code.google.com/archive/p/word2vec/\n",
    "    # File: GoogleNews-vectors-negative300.bin (~3.4 GB)\n",
    "    word2vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "    print(\"Loaded pre-trained Word2Vec model (Mikolov et al.).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Word2Vec file {word2vec_path} not found. Please download from https://code.google.com/archive/p/word2vec/\")\n",
    "    print(\"Alternatively, use gensim.downloader:\")\n",
    "    print(\"import gensim.downloader as api; word2vec_model = api.load('word2vec-google-news-300')\")\n",
    "    exit(1)\n",
    "\n",
    "# Function to create document vectors by averaging Word2Vec embeddings\n",
    "def document_vector(doc, model, embedding_dim=300):\n",
    "    vectors = [model[word] for word in doc if word in model]\n",
    "    if not vectors:  # Handle empty documents or out-of-vocabulary words\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create document vectors\n",
    "doc_vectors = [document_vector(doc, word2vec_model) for doc in tokenized_docs]\n",
    "\n",
    "# Print document vectors (first 5 dimensions for brevity)\n",
    "print(\"\\nDocument Vectors (first 5 dimensions):\")\n",
    "for i, vec in enumerate(doc_vectors):\n",
    "    print(f\"Document {i+1} ({topics[i]}): {vec[:5]}...\")\n",
    "\n",
    "# Logistic Regression Configuration\n",
    "logistic_model = LogisticRegression(\n",
    "    multi_class='multinomial',  # For multi-class classification\n",
    "    solver='lbfgs',  # Suitable for small datasets\n",
    "    C=1.0,  # Regularization strength\n",
    "    max_iter=1000,  # Ensure convergence\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluate using Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for train_index, test_index in loo.split(doc_vectors):\n",
    "    X_train = [doc_vectors[i] for i in train_index]\n",
    "    y_train = [labels[i] for i in train_index]\n",
    "    X_test = [doc_vectors[i] for i in test_index]\n",
    "    y_test = [labels[i] for i in test_index]\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    y_pred = logistic_model.predict(X_test)\n",
    "    predictions.append(y_pred[0])\n",
    "    true_labels.append(y_test[0])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"\\nLeave-One-Out Cross-Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"{'':>20} {'Predicted':>30}\")\n",
    "print(f\"{'':>20} {' '.join([f'{t[:5]:>5}' for t in topics])}\")\n",
    "for i, row in enumerate(conf_matrix):\n",
    "    print(f\"True {topics[i][:5]:>15} {' '.join([f'{val:>5}' for val in row])}\")\n",
    "\n",
    "# Sample prediction\n",
    "sample_doc = tokenized_docs[0]  # Blockchain\n",
    "sample_vector = document_vector(sample_doc, word2vec_model)\n",
    "predicted_label = logistic_model.fit(doc_vectors, labels).predict([sample_vector])[0]\n",
    "print(f\"\\nSample Prediction for Document 1 ({topics[0]}):\")\n",
    "print(f\"Predicted Topic: {topics[predicted_label]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
